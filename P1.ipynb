{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCAPWR2s3nzA"
   },
   "source": [
    "# Data Science Portfolio - Part I (30 marks)\n",
    "\n",
    "In this question you will write Python code for processing, analyzing and understanding the social network **Reddit** (www.reddit.com). Reddit is a platform that allows users to upload posts and comment on them, and is divided in _subreddits_, often covering specific themes or areas of interest (for example, [world news](https://www.reddit.com/r/worldnews/), [ukpolitics](https://www.reddit.com/r/ukpolitics/) or [nintendo](https://www.reddit.com/r/nintendo)). You are provided with a subset of Reddit with posts from Covid-related subreddits (e.g., _CoronavirusUK_ or _NoNewNormal_), as well as randomly selected subreddits (e.g., _donaldtrump_ or _razer_).\n",
    "\n",
    "The `csv` dataset you are provided contains one row per post, and has information about three entities: **posts**, **users** and **subreddits**. The column names are self-explanatory: columns starting with the prefix `user_` describe users, those starting with the prefix `subr_` describe subreddits, the `subreddit` column is the subreddit name, and the rest of the columns are post attributes (`author`, `posted_at`, `title` and post text - the `selftext` column-, number of comments - `num_comments`, `score`, etc.).\n",
    "\n",
    "In this exercise, you are asked to perform a number of operations to gain insights from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Pm74v1u4d6G",
    "outputId": "9411d83e-d90f-465d-b902-c8654feecd4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "# suggested imports\n",
    "import pandas as pd\n",
    "from nltk.tag import pos_tag\n",
    "import re\n",
    "from collections import defaultdict,Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "tqdm.pandas()\n",
    "from ast import literal_eval\n",
    "# nltk imports, note that these outputs may be different if you are using colab or local jupyter notebooks\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WfNsDQ253nzJ",
    "outputId": "6d49e966-1244-41fb-a5f3-7a9fd9935acc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching https://raw.githubusercontent.com/luisespinosaanke/cmt309-portfolio/master/data_portfolio_21.csv\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "import pandas as pd\n",
    "module_url = f\"https://raw.githubusercontent.com/luisespinosaanke/cmt309-portfolio/master/data_portfolio_21.csv\"\n",
    "module_name = module_url.split('/')[-1]\n",
    "print(f'Fetching {module_url}')\n",
    "#with open(\"file_1.txt\") as f1, open(\"file_2.txt\") as f2\n",
    "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
    "  a = f.read()\n",
    "  outf.write(a.decode('utf-8'))\n",
    "\n",
    "\n",
    "df = pd.read_csv('data_portfolio_21.csv')\n",
    "# this fills empty cells with empty strings\n",
    "df = df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "CNfbxg2X3nzK",
    "outputId": "af7c9fac-046c-47cd-de57-a2da8881c64a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-148ff351-3299-45f6-8d58-ba2bf375f56b\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>posted_at</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subr_created_at</th>\n",
       "      <th>subr_description</th>\n",
       "      <th>subr_faved_by</th>\n",
       "      <th>subr_numb_members</th>\n",
       "      <th>subr_numb_posts</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>total_awards_received</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>user_num_posts</th>\n",
       "      <th>user_registered_at</th>\n",
       "      <th>user_upvote_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-Howitzer-</td>\n",
       "      <td>2020-08-17 20:26:04</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2009-04-29</td>\n",
       "      <td>Subreddit about Donald Trump</td>\n",
       "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
       "      <td>30053</td>\n",
       "      <td>796986</td>\n",
       "      <td>donaldtrump</td>\n",
       "      <td>BREAKING: Trump to begin hiding in mailboxes t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4661</td>\n",
       "      <td>2012-11-09</td>\n",
       "      <td>-0.658599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-Howitzer-</td>\n",
       "      <td>2020-07-06 17:01:48</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>2009-04-29</td>\n",
       "      <td>Subreddit about Donald Trump</td>\n",
       "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
       "      <td>30053</td>\n",
       "      <td>796986</td>\n",
       "      <td>donaldtrump</td>\n",
       "      <td>Joe Biden's America</td>\n",
       "      <td>0</td>\n",
       "      <td>0.67</td>\n",
       "      <td>4661</td>\n",
       "      <td>2012-11-09</td>\n",
       "      <td>-0.658599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-Howitzer-</td>\n",
       "      <td>2020-09-09 02:29:02</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2009-04-29</td>\n",
       "      <td>Subreddit about Donald Trump</td>\n",
       "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
       "      <td>30053</td>\n",
       "      <td>796986</td>\n",
       "      <td>donaldtrump</td>\n",
       "      <td>4 more years and we can erase his legacy for g...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4661</td>\n",
       "      <td>2012-11-09</td>\n",
       "      <td>-0.658599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-Howitzer-</td>\n",
       "      <td>2020-06-23 23:02:39</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2009-04-29</td>\n",
       "      <td>Subreddit about Donald Trump</td>\n",
       "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
       "      <td>30053</td>\n",
       "      <td>796986</td>\n",
       "      <td>donaldtrump</td>\n",
       "      <td>Revelation 9:6 [Transhumanism: The New Religio...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4661</td>\n",
       "      <td>2012-11-09</td>\n",
       "      <td>-0.658599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-Howitzer-</td>\n",
       "      <td>2020-08-07 04:13:53</td>\n",
       "      <td>32</td>\n",
       "      <td>622</td>\n",
       "      <td></td>\n",
       "      <td>2009-04-29</td>\n",
       "      <td>Subreddit about Donald Trump</td>\n",
       "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
       "      <td>30053</td>\n",
       "      <td>796986</td>\n",
       "      <td>donaldtrump</td>\n",
       "      <td>LOOK HERE, FAT</td>\n",
       "      <td>0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>4661</td>\n",
       "      <td>2012-11-09</td>\n",
       "      <td>-0.658599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-148ff351-3299-45f6-8d58-ba2bf375f56b')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-148ff351-3299-45f6-8d58-ba2bf375f56b button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-148ff351-3299-45f6-8d58-ba2bf375f56b');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       author            posted_at  num_comments  score selftext  \\\n",
       "0  -Howitzer-  2020-08-17 20:26:04            19      1            \n",
       "1  -Howitzer-  2020-07-06 17:01:48             1      3            \n",
       "2  -Howitzer-  2020-09-09 02:29:02             3      1            \n",
       "3  -Howitzer-  2020-06-23 23:02:39             2      1            \n",
       "4  -Howitzer-  2020-08-07 04:13:53            32    622            \n",
       "\n",
       "  subr_created_at              subr_description  \\\n",
       "0      2009-04-29  Subreddit about Donald Trump   \n",
       "1      2009-04-29  Subreddit about Donald Trump   \n",
       "2      2009-04-29  Subreddit about Donald Trump   \n",
       "3      2009-04-29  Subreddit about Donald Trump   \n",
       "4      2009-04-29  Subreddit about Donald Trump   \n",
       "\n",
       "                                       subr_faved_by  subr_numb_members  \\\n",
       "0  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
       "1  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
       "2  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
       "3  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
       "4  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
       "\n",
       "   subr_numb_posts    subreddit  \\\n",
       "0           796986  donaldtrump   \n",
       "1           796986  donaldtrump   \n",
       "2           796986  donaldtrump   \n",
       "3           796986  donaldtrump   \n",
       "4           796986  donaldtrump   \n",
       "\n",
       "                                               title  total_awards_received  \\\n",
       "0  BREAKING: Trump to begin hiding in mailboxes t...                      0   \n",
       "1                                Joe Biden's America                      0   \n",
       "2  4 more years and we can erase his legacy for g...                      0   \n",
       "3  Revelation 9:6 [Transhumanism: The New Religio...                      0   \n",
       "4                                     LOOK HERE, FAT                      0   \n",
       "\n",
       "   upvote_ratio  user_num_posts user_registered_at  user_upvote_ratio  \n",
       "0          1.00            4661         2012-11-09          -0.658599  \n",
       "1          0.67            4661         2012-11-09          -0.658599  \n",
       "2          1.00            4661         2012-11-09          -0.658599  \n",
       "3          1.00            4661         2012-11-09          -0.658599  \n",
       "4          0.88            4661         2012-11-09          -0.658599  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyQyR27z48nr"
   },
   "source": [
    "## P1.1 - Text data processing (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLUtCUL853Ln"
   },
   "source": [
    "### P1.1.1 - Faved by as lists (3 marks)\n",
    "\n",
    "The column `subr_faved_by` contains an array of values (names of redditors who added the subreddit to which the current post was submitted), but unfortunately they are in text format, and you would not be able to process them properly without converting them to a suitable python type. You must convert these string values to Python lists, going from\n",
    "\n",
    "```python\n",
    "'[\"user1\", \"user2\" ... ]'\n",
    "```\n",
    "\n",
    "to\n",
    "\n",
    "```python\n",
    "[\"user1\", \"user2\" ... ]\n",
    "```\n",
    "\n",
    "**What to implement:** Implement a function `transform_faves(df)` which takes as input the original dataframe and returns the same dataframe, but with one additional column called `subr_faved_by_as_list`, where you have the same information as in `subr_faved_by`, but as a python list instead of a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RJLEddGE56qw"
   },
   "outputs": [],
   "source": [
    "def transform_faves(df):\n",
    "  '''\n",
    "  Function that takes the DataFrame, and adds a column called 'subr_faved_by\n",
    "  as_list' which is the list form of the column 'subr_faved_by'.\n",
    "\n",
    "  Arguments: df - A DataFrame object with column called 'subr_faved_by'.\n",
    "  The column must have string object elements.\n",
    "\n",
    "  Returns: The original DataFrame, but with subr_faved_by_as_list column added.\n",
    "  '''\n",
    "  \n",
    "  # Form temporary list to hold values\n",
    "  temp_list = []\n",
    "\n",
    "  # Loop through subr_faved_by (the strings)\n",
    "  for i in df['subr_faved_by']:\n",
    "    \n",
    "    # Add to temp_list the strings which are stripped of leading and trailing square brackets and remove whitespace and ' characters\n",
    "    new_str = i.replace('[', '').replace(']', '').replace(\"'\", '')\n",
    "    temp_list.append(new_str.split(','))\n",
    "\n",
    "  # Make new column in dataframe with temp_list\n",
    "  df['subr_faved_by_as_list'] = temp_list\n",
    "\n",
    "  # Return new dataframe\n",
    "  return df\n",
    "  \n",
    "df = transform_faves(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhZ3u5aS3rrm"
   },
   "source": [
    "### P1.1.2 - Merge titles and text bodies (4 marks)\n",
    "\n",
    "All Reddit posts need to have a title, but a text body is optional. However, we want to be able to access all free text information for each post without having to look at two columns every time.\n",
    "\n",
    "**What to implement**: A function `concat(df)` that will take as input the original dataframe and will return it with an additional column called `full_text`, which will concatenate `title` and `selftext` columns, but with the following restrictions:\n",
    "\n",
    "- 1) Wrap the title between `<title>` and `</title>` tags.\n",
    "- 2) Add a new line (`\\n`) between title and selftext, but only in cases where you have both values (see instruction 4).\n",
    "- 3) Wrap the selftext between `<selftext>` and `</selftext>`.\n",
    "- 4) You **must not** include the tags in points (1) or (3) if the values for these columns is missing. We will consider a missing value either an empty value (empty string) or a string of only one character (e.g., an emoji). Also, the value of a `full_text` column must not end in the new line character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RsmY-JB39N2m"
   },
   "outputs": [],
   "source": [
    "def concat(df):\n",
    "  '''\n",
    "  Function that takes a data frame, and merges the title and selftext columns.\n",
    "  The function also adds html <title> and <selftext> tags provided the length\n",
    "  of title or selftext is greater than one.\n",
    "\n",
    "  Arguments: df: DataFrame containing 'title' and 'selftext' columns.\n",
    "\n",
    "  Returns: df: Original DataFrame with 'full_text' column added. This full_text\n",
    "  is the merging of title and selftext, with html tags added.\n",
    "  '''\n",
    "\n",
    "  # Define empty list to holder title and selftext\n",
    "  temp_list = []\n",
    "\n",
    "  # Loop through both title and selftext columns, adding tags if necessary\n",
    "  for i in range(len(df['title'])):\n",
    "    if len(df['title'][i]) > 1 and len(df['selftext'][i]) > 1: # Both title and selftext has characters\n",
    "      title_selftext = '<title>' + df['title'][i] + '</title>' + '\\n' + '<selftext>' + df['selftext'][i] + '</selftext>'\n",
    "      temp_list.append(title_selftext)\n",
    "    elif len(df['title'][i]) > 1 and len(df['selftext'][i]) <= 1: # Only title has characters\n",
    "      title_only = '<title>' + df['title'][i] + '</title>' \n",
    "      temp_list.append(title_only)\n",
    "    elif len(df['title'][i]) <=1 and len(df['selftext'][i]) > 1: # Only selftext has characters\n",
    "      selftext_only = '<selftext>' + df['selftext'][i] + '</selftext>'\n",
    "      temp_list.append(selftext_only)\n",
    "    else: # In the event there is only a single character in the title or selftext\n",
    "      temp_list.append('')\n",
    "\n",
    "  # Make new column to store temp_list entries\n",
    "  df['full_text'] = temp_list\n",
    "\n",
    "  # Return df\n",
    "  return df\n",
    "\n",
    "df = concat(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADWvbAIe4TVd"
   },
   "source": [
    "### P1.1.3 - Enrich posts (3 marks)\n",
    "\n",
    "We would like to augment our text data with linguistic information. To this end, we will _tokenize_, apply _part-of-speech tagging_, and then we will _lower case_ all the posts.\n",
    "\n",
    "**What to implement**: A function `enrich_posts(df)` that will take as input the original dataframe and will return it with **two** additional columns: `enriched_title` and `enriched_selftext`. These columns will contain tokenized, pos-tagged and lower cased versions of the original text. **You must implement them in this order**, because the pos tagger uses casing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_nDnaSwI46T_"
   },
   "outputs": [],
   "source": [
    "def enrich_posts(df):\n",
    "  '''\n",
    "  Function that tokenizes, POS_tags, and lower cases words in that order. \n",
    "  Designed to be used specifically on 'title' and 'selftext' columns of \n",
    "  dataframe.\n",
    "\n",
    "  Arguments: df: DataFrame containing 'title' and 'selftext' columns.\n",
    "\n",
    "  Returns: df: Original DataFrame but with two columns added, namely 'enriched\n",
    "  _title' and 'enriched_title'\n",
    "\n",
    "  '''\n",
    "\n",
    "  # Write  helper function to lower case first entries in a list of tuples\n",
    "  def lower_case(list_tuples):\n",
    "    '''\n",
    "    Helper function designed to be used in conjuction with enrich_posts() function.\n",
    "    Takes a list of tuples, and lower cases every first entry in the tuples.\n",
    "\n",
    "    Arguments: list_tuples: A list of tuples.\n",
    "\n",
    "    Returns: holder: A list of tuples, but with the first entry of every tuple\n",
    "    in lower case.\n",
    "    '''\n",
    "\n",
    "    # Define new list to hold entries\n",
    "    holder = []\n",
    "\n",
    "    # Convert first entry to lowercase\n",
    "    for i in list_tuples:\n",
    "      temp = list(i)\n",
    "      temp[0] = temp[0].lower()\n",
    "      holder.append(tuple(temp))\n",
    "\n",
    "    # Return holder array\n",
    "    return holder\n",
    "  \n",
    "  # Tokenize and POS tag title\n",
    "  title = map(nltk.word_tokenize, list(df['title']))\n",
    "  title = map(nltk.pos_tag, list(title))\n",
    "  title = map(lower_case, list(title))\n",
    "  title = list(title)\n",
    "\n",
    "  # Tokenize and POS tag selftext\n",
    "  selftext = map(nltk.word_tokenize, list(df['selftext']))\n",
    "  selftext = map(nltk.pos_tag, list(selftext))\n",
    "  selftext = map(lower_case, list(selftext))\n",
    "  selftext = list(selftext)\n",
    "\n",
    "  # Add enriched_title and enriched_selftext columns to dataframe\n",
    "  df['enriched_title'] = title\n",
    "  df['enriched_selftext'] = selftext\n",
    "\n",
    "  return df\n",
    "\n",
    "df = enrich_posts(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8E010UbQyML"
   },
   "source": [
    "## P1.2 - Answering questions with pandas (12 marks)\n",
    "\n",
    "In this question, your task is to use pandas to answer questions about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZmG2VIYQ93I"
   },
   "source": [
    "### P1.2.1 - Users with best scores (3 marks)\n",
    "\n",
    "- Find the users with the highest aggregate scores (over all their posts) for the whole dataset. You should restrict your results to only those whose aggregated score is above 10,000 points, in descending order. Your code should generate a dictionary of the form `{author:aggregated_scores ... }`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RhW8Rr6QSXDj",
    "outputId": "068a9078-bdf8-49ee-e701-73e1f77f571e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DaFunkJunkie': 250375, 'None': 218846, 'SUPERGUESSOUS': 211611, 'jigsawmap': 210824, 'chrisdh79': 143538, 'hildebrand_rarity': 122464, 'iSlingShlong': 118595, 'hilltopye': 81245, 'tefunka': 79560, 'OldFashionedJizz': 64398, 'JLBesq1981': 58235, 'rspix000': 57107, 'Wagamaga': 47989, 'stem12345679': 47455, 'TheJeck': 26058, 'TheGamerDanYT': 25357, 'TrumpSharted': 21154, 'NotsoPG': 18518, 'SonictheManhog': 18116, 'BlanketMage': 13677, 'NewAltWhoThis': 12771, 'kevinmrr': 11900, 'Dajakesta0624': 11613, 'apocalypticalley': 10382}\n"
     ]
    }
   ],
   "source": [
    "# Group by author and sum scores\n",
    "authors = df.groupby('author')\n",
    "scores = authors['score'].sum()\n",
    "\n",
    "# Make new list of scores above 10,000\n",
    "scores_aggregated = scores[scores > 10000]\n",
    "\n",
    "# Sort values\n",
    "scores_aggregated = scores_aggregated.sort_values(ascending = False)\n",
    "\n",
    "# Form dictionary\n",
    "aggregated_scores_authors = dict(scores_aggregated)\n",
    "\n",
    "# Show dictionary\n",
    "print(aggregated_scores_authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woOFrPFQT5cZ"
   },
   "source": [
    "### P1.2.2 - Awarded posts (3 marks)\n",
    "\n",
    "Find the number of posts that have received at least one award. Your query should return only one value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fVuaWmmUGVW",
    "outputId": "57d4c628-9c74-49aa-cc52-693f63456e65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n"
     ]
    }
   ],
   "source": [
    "# Find awarded posts\n",
    "amount_of_awards = 0\n",
    "for i in df['total_awards_received']:\n",
    "  if i > 0:\n",
    "    amount_of_awards += 1\n",
    "\n",
    "print(amount_of_awards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVj1WikSUPjO"
   },
   "source": [
    "### P1.2.3 Find Covid (3 marks)\n",
    "\n",
    "Find the name and description of all subreddits where the name starts with `Covid` or `Corona` and the description contains `covid` or `Covid` anywhere. Your code should generate a dictionary of the form#\n",
    "\n",
    "```python\n",
    "  {'Coronavirus':'Place to discuss all things COVID-related',\n",
    "  ...\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w6fIWO8BUhu3",
    "outputId": "50d815a2-6a14-4f25-84f7-125a60d80d0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COVID': 'COVID-19 News, Etc.',\n",
       " 'COVID19': 'In December 2019, SARS-CoV-2, the virus causing the disease COVID-19, emerged in the city of Wuhan, China. This subreddit seeks to facilitate scientific discussion of this global public health threat.',\n",
       " 'Coronavirus': 'Place to discuss all things COVID-related',\n",
       " 'CoronavirusCA': 'Tracking the Coronavirus/Covid-19 outbreak in California',\n",
       " 'CoronavirusDownunder': 'This subreddit is a place to share news, information, resources, and support that relate to the novel coronavirus SARS-CoV-2 and the disease it causes called COVID-19. The primary focus of this sub is to actively monitor the situation in Australia, but all posts on international news and other virus-related topics are welcome, to the extent they are beneficial in keeping those in Australia informed.',\n",
       " 'CoronavirusUS': 'USA/Canada specific information on the coronavirus (SARS-CoV-2) that causes coronavirus disease 2019 (COVID-19)'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create unique subreddits and description names\n",
    "subreddits_unique = pd.unique(df['subreddit'])\n",
    "descriptions_unique = pd.unique(df['subr_description'])\n",
    "\n",
    "# Make new list to hold coronavirus related subreddit entries\n",
    "corona_subreddits = {}\n",
    "\n",
    "# Loop through subreddits and descriptions identifying covid/corona, and enter into list of tuples\n",
    "for i in range(len(subreddits_unique)):\n",
    "  if (subreddits_unique[i].lower().count('covid') > 0 or subreddits_unique[i].lower().count('corona')) > 0 and descriptions_unique[i].lower().count('covid') > 0:\n",
    "    corona_subreddits.update({subreddits_unique[i] : descriptions_unique[i]})\n",
    "\n",
    "# Show corona_subreddits\n",
    "corona_subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToPttp2-fsXG"
   },
   "source": [
    "### P1.2.4 - Redditors that favorite the most\n",
    "\n",
    "Find the users that have favorited the largest number of subreddits. You must produce a pandas dataframe with **two** columns, with the following format:\n",
    "\n",
    "```python\n",
    "     redditor\t    numb_favs\n",
    "0\tuser1           7\n",
    "1\tuser2           6\n",
    "2\tuser3\t       5\n",
    "3\tuser4           4\n",
    "...\n",
    "```\n",
    "\n",
    "where the first column is a Redditor username and the second column is the number of distinct subreddits he/she has favorited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "LbFeie3jip44",
    "outputId": "41c4ed9d-5899-4452-a660-7296bb67c7aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1650\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-1071dc3b-335e-40dd-b442-ac93f983bf74\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>redditor</th>\n",
       "      <th>numb_faves</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>magnusthered15</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KarmaFury</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FriendlyVegetable420</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OmniusQubus</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hmhmhm2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>certifiedloverboy69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1646</th>\n",
       "      <td>diveonfire</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>mouthofreason</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648</th>\n",
       "      <td>Alexify</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649</th>\n",
       "      <td>lilstinky***</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1650 rows × 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1071dc3b-335e-40dd-b442-ac93f983bf74')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-1071dc3b-335e-40dd-b442-ac93f983bf74 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-1071dc3b-335e-40dd-b442-ac93f983bf74');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                   redditor  numb_faves\n",
       "0            magnusthered15           7\n",
       "1                 KarmaFury           6\n",
       "2      FriendlyVegetable420           6\n",
       "3               OmniusQubus           6\n",
       "4                   hmhmhm2           6\n",
       "...                     ...         ...\n",
       "1645    certifiedloverboy69           1\n",
       "1646             diveonfire           1\n",
       "1647          mouthofreason           1\n",
       "1648                Alexify           1\n",
       "1649           lilstinky***           1\n",
       "\n",
       "[1650 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get subreddits and remove extra values\n",
    "subreddits_faved_by = pd.Series(df['subr_faved_by_as_list'])\n",
    "subreddits_faved_by = list(subreddits_faved_by.drop_duplicates())\n",
    "\n",
    "# Flatten list\n",
    "subreddits_faved_flat = [item for sublist in subreddits_faved_by for item in sublist]\n",
    "\n",
    "# Form unique users list\n",
    "usernames_unique = list(pd.unique(subreddits_faved_flat))\n",
    "print(len(usernames_unique))\n",
    "# Compare usernames_unique against subreddits_faved_flat\n",
    "favourites = []\n",
    "for i in usernames_unique:\n",
    "  favourites.append(subreddits_faved_flat.count(i))\n",
    "\n",
    "# Form dictionary\n",
    "users = dict(zip(usernames_unique, favourites))\n",
    "\n",
    "# Sort users\n",
    "users_sorted = {k: b for k, b in sorted(users.items(),\n",
    "                key = lambda i: i[1], reverse = True)}\n",
    "\n",
    "# Form dataframe\n",
    "df_redditors = pd.DataFrame(list(users_sorted.items()), columns = ['redditor', 'numb_faves'])\n",
    "\n",
    "# Show dataframe\n",
    "df_redditors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsAF9jpblJLp"
   },
   "source": [
    "## P1.3 Ethics (8 marks)\n",
    "\n",
    "**(updated on 16/03/2022)**\n",
    "\n",
    "Imagine you are **the head of a data mining company that needs to use** the insights gained in this assignment to scan social media for covid-related content, and automatically flag it as conspiracy or not conspiracy (for example, for hiding potentially harmful tweets or Facebook posts). **Some information about the project and the team:**\n",
    "\n",
    " - Your client is a political party concerned about misinformation.\n",
    " - The project requires mining Facebook, Reddit and Instagram data.\n",
    " - The team consists of Joe, an American mathematician who just finished college; Fei, a senior software engineer from China; and Francisco, a data scientist from Spain.\n",
    "\n",
    "Reflect on the impact of exploiting data science for such an application. You should map your discussion to one of the five actions outlined in the UK’s Data Ethics Framework. \n",
    "\n",
    "Your answer should address the following:\n",
    "\n",
    " - Identify the action **in which your project is the weakest**.\n",
    " - Then, justify your choice by critically analyzing the three key principles **for that action** outlined in the Framework, namely transparency, accountability and fairness.\n",
    " - Finally, you should propose one solution that explicitly addresses one point related to one of these three principles, reflecting on how your solution would improve the data cycle in this particular use case.\n",
    "\n",
    "Your answer should be between 500 and 700 words. **You are strongly encouraged to follow a scholarly approach, e.g., with references to peer reviewed publications. References do not count towards the word limit.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjlaJYL-LkxH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YJQSO8Amuea"
   },
   "source": [
    "---\n",
    "\n",
    "The specific actions of the UK Data Ethics Framework include ‘Define and understand public benefit and user need’, ‘Involve diverse expertise’, ‘Comply with the law’, ‘Review the quality and limitations of the data’, and ‘Evaluate and consider wider policy implications’ [1]. \n",
    "\n",
    "\n",
    "The project aims to flag false news about the COVID pandemic, which will allow users to be able to avoid false information, the public benefit being that events such as phone mast vandalism [2] and other bizarre conspiracies will occur less if false information is not spread online. The team consists of three different nationalities, all with varying skillsets and experience, showing that the team has diverse expertise, which is important for the project reaching its full potential [3]. Legally, there is not a great involvement; the team does not have a legal advisor nor has the team received any legal advice. In terms of the quality of the data, there are assessments to assess quality of data [4], however this has not been used in this case – we can say that the data for the project is representative of most of the public, with a median age range of 25 – 34 on Facebook and Instragram [5,6], and a median range of 30 – 49 on Reddit [7]. Due to these ranges covering a fair amount of the adult population, it is a fair assessment to say that the data is representative of most of the public. In terms of evaluation and policy considerations, as the project is yet to be completed there is little to evaluate. However, we can say that there is plan for future actions, namely the flagging of COVID conspiracy content, but that is all at this current point in time.\n",
    "\n",
    "\n",
    "Clearly, the project’s legal aspect is the weakest element of it, as generally little consideration has been given to this aspect. In terms of accountability, the team would do well to ensure they have a technically minded legal advisor, as the personal data must comply with the EU General Data Protection Regulation (GDPR) and the Data Protection Act 2018 (DPA) [1]. The GDPR is extremely crucial for protecting the freedoms of the individual and restricting the free movement of this data [8]. Given that the team has no legal advisors nor external legal support, it will be extremely difficult to comply with any such laws. This is an example of the team being inadequate in the accountability portion of the legal aspect of things. \n",
    "\n",
    "\n",
    "The team should also aim to publish the DPIA as a part of transparency, and as it ‘is a legal requirement for any type of processing’ [9] and could lead to hefty fines if this is ignored. The team must also publish other related documents to the DPIA; the team would require legal advice to do so. At this point in time, no reference has been made to the DPIA, showing a lack of transparency within the legal aspect of the team. \n",
    "\n",
    "\n",
    "In terms of fairness, the project must comply with the Equality Act 2010, such that ‘Data analysis or automated decision making must not result in outcomes that lead to discrimination’ [1]. There is no equality officer within the team, and ‘public bodies are responsible for ensuring that any third parties which exercise functions on their behalf are capable of complying with the Equality Duty’ [10], so this could lead to issues for the political party commissioning the project.\n",
    "\n",
    "\n",
    "To solve the accountability of the legal aspect of this project (and perhaps indirectly aid transparency and fairness) the team should seek to obtain the services of a legal advisor, who could then aid the team in complying with the GDPR. Not only would this prevent the team (and political party) from repercussions from not following the law, but it would also add to the accountability such that the public could have confidence that their data was being used appropriately. This would lead to more public support for the project, as if the project did not have this accountability, the public would not know how their data was being used, and could lead to poor support for the project, or even to conspiracy theorists becoming further entrenched in their views. \n",
    "\n",
    "\n",
    "\n",
    "References\n",
    "[1] - UK Data Ethics Framework, British Government. Available at: https://www.gov.uk/government/publications/data-ethics-framework/data-ethics-framework-2020 . [Accessed 04/05/2022]\n",
    "\n",
    "\n",
    "[2] – Mast fire probe amid 5G coronavirus claims, BBC News. Available at: https://www.bbc.co.uk/news/uk-england-52164358 . [Accessed 04/05/2022]\n",
    "\n",
    "\n",
    "[3] – 4 Lessons for Building Diverse Teams, Naomi Wheeles, Harvard Business Review. Available at: https://hbr.org/2021/05/4-lessons-for-building-diverse-teams . [Accessed 04/05/2022].\n",
    "\n",
    "\n",
    "[4] - Cai, L. and Zhu, Y., 2015. The Challenges of Data Quality and Data Quality Assessment in the Big Data Era. Data Science Journal, 14, p.2.\n",
    "\n",
    "\n",
    "[5] – Distribution of Facebook users worldwide as of January 2022, by age and gender. Available at: https://www.statista.com/statistics/376128/facebook-global-user-age-distribution/ . [Accessed 04/05/2022].\n",
    "\n",
    "\n",
    "[6] – Distribution of Instagram users worldwide as of January 2022, by age and gender. Available at: https://www.statista.com/statistics/248769/age-distribution-of-worldwide-instagram-users/ . [Accessed 04/05/2022].\n",
    "\n",
    "\n",
    "[7] – The Demographics of Reddit: Who Uses The Site? William Sattelberg, Alphr. Available at: https://www.alphr.com/demographics-reddit/ . [Accessed 04/05/2022]. \n",
    "\n",
    "\n",
    "[8] – G. Chassang, The impact of the EU general data protection regulation on scientific research, Ecancer medical Science, 2017.\n",
    "\n",
    "\n",
    "[9] – What is a DPIA, Information Commissioner’s Office. Available at: https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/data-protection-impact-assessments-dpias/what-is-a-dpia/ . [Accessed 04/05/2022].\n",
    "\n",
    "\n",
    "[10] – Public Sector Equality Duty – A Guide To Meeting The Duty And Undertaking Equality Analysis, PSED Toolkit Appendix 3, City of London.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "P1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
